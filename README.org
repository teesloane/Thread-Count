** Table of Contents :TOC_3:
  - [[#introduction-and-overview][Introduction and Overview]]
  - [[#why-am-i-writing-this][Why am I writing this?]]
  - [[#on-whos-authority][On who's authority?]]
  - [[#starting-definitions---threads-and-procesess][Starting definitions - Threads and Procesess]]
  - [[#processors][Processors]]
  - [[#back-to-threads][Back to threads]]
    - [[#my-first-thread][My first Thread!]]
    - [[#my-first-program-with-more-than-one-thread][My first program with more than one thread]]
  - [[#when-would-i-use-threads-why-would-i-want-to-write-a-multi-threaded-program][When would I use threads? Why would I want to write a multi-threaded program?]]
    - [[#a-shift-towards-parallel-programming-as-a-standard][A shift towards parallel programming as a standard?]]
  - [[#why-is-it-often-difficult-to-write-multi-threaded-programs][Why is it often difficult to write multi-threaded programs?]]
    - [[#race-conditions][Race Conditions]]
    - [[#shared-mutable-state][Shared mutable state]]
  - [[#concurrency-models-csp-actor-model-stm][Concurrency Models (CSP, Actor Model, STM)]]
    - [[#actor-model][Actor Model]]
    - [[#csp][CSP]]
    - [[#software-transactional-memory][Software Transactional Memory]]
  - [[#concurrency-vs-paralellism][Concurrency Vs Paralellism]]
  - [[#research-and-resources][Research and resources]]

** Introduction and Overview

This doc is an attempt to chart my own deep dive into understanding concurrent and parallel programming. This document will be a constant work and progress. Lots to learn. Lots to cover.

** Why am I writing this?

I started programming by learning Javascript, outside of a more "traditional" comp-sci/comp-eng setting; I followed online resources, courses, and built projects in my spare time (I still do!). In the last few months I've been learning new languages; languages where words like ~threads~ ~concurrency~ ~parallelism~ ~futures~ ~race-conditions~ and so on, tend to show up. And me? I'm like a leaf in the wind, floating by and circling around these topics -- picking up little notions of what they mean as I go. These ancillary understandings were enough for me to get by while I built my projects, went to work, read articles, and listened to podcasts about new programming languages.

However! I can only exist like a ghost on the edge of understanding for so long before I become frustrated; I have been building a puzzle for this new knowledge, but I have only found the borders (the easiest pieces to find!) to frame a new picture, yet I have not filled out the middle.

What I'm trying to say, is that I want to do a deep dive into this topic; this doc is my attempt to fill in the center of the puzzle that is this world of understanding concurrency, parallel programming, and /whatever-you-call-its/ (surely by the end of my research I'll have a better umbrella term for all this; for now, you'll have to bear with me.) While I'm a firm advocate of learning by building; I also enjoy doing these deep-dives on occaision (especially when I'm circling around a topic but not engaging directly with it)

NB: /A javascript-first education provides a strange and intriguing position to approach these topics. In my discussions I've often struggled to relate these new topics to my perceived knowledge in the world of programming, framed by having learned javascript _first_. Is there anything wrong with this? No! But, I'm writing this novella here because I've found that my position has not aligned as well as I'd hoped to understand some of these concepts./

NB: /While I do claim this doc is for my own purposes, I've also tried to make it more interesting (tone wise) to others than myself. Let's keep it light and have some fun./

** On who's authority?

*Q:* Why should I listen / heed / pay attention to you / this article / all the / slashes you use / ?

*A:* Good question! It's really up to you. This doc is a personal chronicle--an auto-didactic log to fill in my own knowledge gaps. I am publishing this online to share my findings / my take on things.  I would be very pleased if this doc proved useful for people who find themselves in a similar situation as I've described above.

*Q:* Speaking of being wrong, you made a mistake!

*A:* Let's fix that! Feel free to make a PR / open an issue if I've misquoted something (or not properly attributed something), or have a fact wrong. With your help, the vaccuum room of my brain and this outputted mud-doc can shape itself into a gleaming mountain, with resilient flowers in the windy valleys below.

NB: /Since this is a personal doc charting my own understanding, I will gladly consider pull requests, but may incorporate contributions with adaptation in my own writing. Don't let that stop you from making PR's and issues, though./

*Q:* Do you have any other disclaimers for us?

Oh yeah! /Nobody/ is paying me to include /any/ of the links or references to books or whatever. If you want to pay me, FORGET IT! Go and look at some flowers, pet a puppy or paint a painting. I'll know and we can mentally high five across space and time.

Furthermore! If I've incorrectly cited your work /please/ let me know and I can make changes / remove it as you like!

** Starting definitions - Threads and Procesess

A lot of the words and terms I've been throwing around are related to multi-threaded programming. So, let's get started by working up to figure out what a _thread_ is.

At first I thought we could start with this [[https://en.wikipedia.org/wiki/Thread_(computing)][definition from wikipedia]].

#+BEGIN_QUOTE
In computer science, a thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler, which is typically a part of the operating system.[1] The implementation of threads and processes differs between operating systems, but in most cases a thread is a component of a process. Multiple threads can exist within one process, executing concurrently and sharing resources such as memory, while different processes do not share these resources. In particular, the threads of a process share its executable code and the values of its variables at any given time.
#+END_QUOTE

Isn't that quite a bit of information at once though? Let's dig into it.

Let's do something practical. If you open your `terminal` application (and are running unix) you can type in the following to see some processes:

~$ top~

Here's the result of mine:

#+ATTR_HTML: :style margin-left: auto; margin-right: auto;
[[/images/top-output.png]]

WOOP that's a lot of information. But you know what it reminds me of? The activity monitor on my mac -- which I have seen many times before:


#+ATTR_HTML: :style margin-left: auto; margin-right: auto;
[[/images/activity-monitor.png]]

There's a lot of information we can glean from these screenshots (especially if you know what to look for!). Looking closer at my ~top~ output, I can see the following: ~Processes: 391 total, 2 running, 389 sleeping, 2309 threads~

Hmm, I just quit my twitter client and now I have this: ~Processes: 389 total, 3 running, 386 sleeping, 2296 threads~.

Hmm (pt 2)... I just quit a project I was working on where I had a terminal pane opened that was running a clojure/clojurescript project (which uses Java). Now the following:

~Processes: 372 total, 2 running, 370 sleeping, 2203 threads~

Alright, alright, contrived / heavy handed examples aside! But now we can deduce that the things running on your computer are /Processes/ -- your browser, your twitter client, your code editor, your terminal, everything you interact with -- All processes! As Wikipedia puts it:

#+BEGIN_QUOTE
A computer program is a passive collection of instructions, while a process is the actual execution of those instructions.
- [[https://en.wikipedia.org/wiki/Process_(computing)][Process (computing)]]
#+END_QUOTE

We're talking about LIVING, BREATHING, PROCESSES PEOPLE. It's beautiful. Processes are identified by a ~PID~ -- a process id. Have you ever had to force quit a program? Maybe your browser froze, so you opened your task manager or activity monitor and forced a process to end. The equivalent of doing that in the terminal is to send a message to kill a process. If I type ~kill 93835~ it's going to kill something -- Google Chrome to be exact (or at least, /a tab in google chrome/ becasue each tab lives in it's /own/ process).

By the way, I DID type in ~kill 93835~ WITHOUT knowing what it was (because I live recklessly). Then this notification showed up:


#+ATTR_HTML: :style margin-left: auto; margin-right: auto;
[[/images/kill-chrome-process.png]]

My beloved vimium extension crashed. Chrome runs each extension as it's own process too, it seems. Neat. Maybe you /should not/ do this though.

Anyway, from these processes we can also see that each one contains a certain amount of running /threads/. On my machine, two instances of Java have 28 threads. What are those 28 threads doing? I don't know! I don't even know if I can figure it out. JAVA! What are you DOING. Maybe I will know at the end of all this research. Now, if we were to return to the above wikipedia quote on threads, it might make a bit more sense.

"Multiple threads can exist within one process, executing concurrently and sharing resources such as memory"

That's where things start to get tricky. I think we're about to get into the material.

But first a quick preface:

** Processors

The /Processor/ is the brain in your computer makin' stuff happen. These days, computers are usually /multi-core-processors/ -- computers with more than one brain, in a sense.

Two brains you say? Or yet, 4, or maybe even 8? "How can a computer work with more than one brain?" you ask. "I can barely operate with 1!" I yell loudly into the void, not answering your question. I bow to the 8 core computer, who is 8x more than me in everyway possible. But here's the ticket (I say, whispering), the 2, 4, 8, 16, whatever-many-brain-ed computer is only as smart as the programs fed to it; and how they make use of multi-core-threading, concurrency, and all those other cool terms I buzzed on about before.

** Back to threads

I think it's about time we looked at some code so that we can take a practical approach to learning how to use threads. Why don't we create a thread?

NB: /Please note that the following examples were built to run on *my machine*. I'd recommend you treat them as reading material./

*** My first Thread!

#+BEGIN_SRC C
#include <stdio.h>

int main() {
    printf("Lucky me, lucky mud.");
    return 0;
}
#+END_SRC

"Hmmm that's strange" You say, "I don't see anything about threads in that program". I look back at you, grinning like a fool. I've laid some trick code before you (like a trick question, but with code right?). The above code is a C program. After compiling it and running it in the command line it prints "Lucky me, lucky mud." While there is no explicit mention of /threads/ the ~main()~ function in this program /is/ a thread; a single default thread. If you want to make more threads you have to do it manually. So let's /actually/ do that (sorry).

*** My first program with more than one thread

#+BEGIN_SRC c
#include <pthread.h>
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>

void *takeNap(void *threadid) {
    // pull the thread name off of the function param
    int tid;
    tid = (int)threadid;

    printf("'Allo It me, thread #%d!\nI like napping, so I'm going to go sleep for 2 seconds \n", tid);
    sleep(2);
    printf("THREAD 1 HERE, I'm BACK, SICK nap, I'M like, gonna STOP EXISTING NOW\n");
    pthread_exit(NULL);
}

int main () {
    pthread_t myThread; // look! A name for our beautiful child to be. So proud.
    int rdata;          // our thread might return an error so we should capture that too.
    int counter;        // we use this for a FOR loop below.


    // Let's make that cake. I mean thread.
    rdata = pthread_create(&myThread, NULL, takeNap, (void *) 1);

    // Let's check in case our new thread returns an error code.
    if (rdata) {
        printf("ERROR: pthread_create() returned an error code of %d\n", rdata);
        exit(-1);
    }

    // Let's make the main function do some work tho right? Let's count some HATS OK?
    for(counter=0; counter<5; counter++){
       sleep(1);
       printf("I see %d cool hats\n", counter);
    }

    // we should exit all our threads I guess.
    pthread_exit(NULL);
}

#+END_SRC

Here's our output:

#+BEGIN_SRC
Hello World! It's me, thread #1!
I like napping, so I'm going to go sleep for 2 seconds
I see 0 cool hats
THREAD 1 HERE, I'm BACK,  SICK nap, I'M like, gonna STOP EXISTING NOW
I see 1 cool hats
I see 2 cool hats
I see 3 cool hats
I see 4 cool hats
#+END_SRC

Wow, so beautiful. Can't stop crying at this. Do you see how two things are happening in parallel? Our ~main()~ function (which is the default single thread of a program) is counting how many cool hats it sees. (4! Four C00L hats believe it or not!). While counting away at some sickening hats, a thread has shot off from the main function and is doing it's own thing -- running the function ~takeNap~.

I've peppered in a bunch of ~sleep()~ functions everywhere to make things more evident if you were to run this on your machine. ~sleep~ pauses the execution of /a thread/ for a certain duration.

Let's address some possible questions from the code above.

*What's up with "pthread"?*

Pthreads are POSIX threads -- threads that adhere to the POSIX standard. I think the following [[https://computing.llnl.gov/tutorials/pthreads/#Pthread][in-depth tutorial]] does a good job summarizing what that means:


#+END_SRC

#+BEGIN_QUOTE
"Historically, hardware vendors have implemented their own proprietary versions of threads. These implementations differed substantially from each other making it difficult for programmers to develop portable threaded applications. [...] Most hardware vendors now offer Pthreads in addition to their proprietary API's."

- [[https://computing.llnl.gov/tutorials/pthreads/#Pthread][POSIX Threads Programming]]
#+END_QUOTE

*What are the params for pthread_create doing?*

If you go to your terminal and type ~man pthread~ you can see a detailed manual for pthread. If we look for the pthread_create function we find:


#+BEGIN_SRC

Thread Routines
   int pthread_create(pthread_t *thread, const pthread_attr_t *attr, void *(*start_routine)(void *), void *arg)
           Creates a new thread of execution.
#+END_SRC

To me, that doesn't read super easily. Let's dig in. Things inbetween ~<~ ~>~ are arguments to the pthread_create function:


*<pthread_t *thread>*
This was the unique thread we declared ("myThread")

*<const pthread_att_t *attr>*
An object housing configuration details for the creation of our thread.

*<start_routine>*
The function we want the thread to start once it's created.

*<arg>*
We can pass a single argument to the starting routine (or NULL)


*So like, what IS a thread?*

So you noticed that I haven't given you my definition of a thread. I'm not going to lie; putting the concept of a thread into [[https://stackoverflow.com/questions/5201852/what-is-a-thread-really][simplified and concrete]] terms is difficult. Currently, at my level of understanding, I don't think I can describe the literal and physical characteristics of a thread; for now  analogy and metaphor serves me better for my own understanding.

I like the idea of Google Docs as a metaphor for threads. In a Google Doc there is the ability for multiple people to edit a document's contents simulataneously (what a feat!). One person may have created the doc (the main thread from the program itself), but they invited 6 other people (created 6 threads) to read and *edit* the document (and in real-time!). Each person has access to the contents (data) of the Google Doc, and can make changes.

I see threads as being closely linked with functions. When we created a thread above, we tied it's execution /to/ a function. Kicking off a thread resulted in a function being invoked; it's just that /that/ function was able to run at the same time as the execution of other code. This thread-function can run in parallel to other code, and like other functions, is able to access it's own local state; global state, and do all the other cool things that functions do.

** When would I use threads? Why would I want to write a multi-threaded program?

Generally, you'd use threads to build faster programs.

Threads enable you to do parallel programming. You can make programs that run faster on hardware with multiple cores. Picking when you do / don't need to take advantages of threads is a case by case scenario, and generally something you want to carefully consider (see "why is it often difficult to write multi-threaded programs?" ahead).

Here are a handful of places you might find threads being used:
- handling lots of I/O (input/output) -- maybe you're reading or writing large amounts of data to disk and want to simultaneously perform other operations.
- handling http requests in a web server.
- you want to prioritize work in order of importance.

*** A shift towards parallel programming as a standard?

In general, multi-core programming has been gaining a lot of interest (as well as languages that purport to make it easier to write / maintain parallel programs than it has been in the past). In 2005 Herb Sutter published "The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software" which details a prescient acknowledgement of coming changes:

#+BEGIN_QUOTE
"If you’re a software developer, chances are that you have already been riding the “free lunch” wave of desktop computer performance. Is your application’s performance borderline for some local operations? “Not to worry,” the conventional (if suspect) wisdom goes; “tomorrow’s processors will have even more throughput, and anyway today’s applications are increasingly throttled by factors other than CPU throughput and memory speed (e.g., they’re often I/O-bound, network-bound, database-bound).” Right?
Right enough, in the past. But dead wrong for the foreseeable future.

The good news is that processors are going to continue to become more powerful. The bad news is that, at least in the short term, the growth will come mostly in directions that do not take most current applications along for their customary free ride.

[...]

Starting today, the performance lunch isn’t free any more. Sure, there will continue to be generally applicable performance gains that everyone can pick up, thanks mainly to cache size improvements. But if you want your application to benefit from the continued exponential throughput advances in new processors, it will need to be a well-written concurrent (usually multithreaded) application. And that’s easier said than done, because not all problems are inherently parallelizable and because concurrent programming is hard.

- https://www.cs.utexas.edu/~lin/cs380p/Free_Lunch.pdf
#+END_QUOTE

Stutter's paper was written in 2005; it's interesting to read [[https://softwareengineering.stackexchange.com/questions/212916/is-the-free-lunch-over][discussions]] and [[http://henrikeichenhardt.blogspot.ca/2013/06/free-lunch-for-programers.html][reflections]] on it, especially years later. Katherine Kox's recently publised book "Concurrency in Go" launches right into these topics within the first chapter with the title "Moore’s Law, Web Scale, and the Mess We’re In". The chapter provides a succinct and descriptive summary of the history of Moore's law and how it relates to concurrency and parallel programming today.

** Why is it often difficult to write multi-threaded programs?

We've gotten a few terms covered (threads, processes, parallel programming) and now I want to talk about /why/ multicore programming can be challenging. We'll start with the concrete example of a "race condition" and then talk a little more abstractly about /shared mutable state/.

*** Race Conditions

Let's check in with our friend Wikipedia to see how digestible the [[https://en.wikipedia.org/wiki/Race_condition][entry on "Race conditions"]] is:

#+BEGIN_QUOTE
A race condition or race hazard is the behavior of an electronics, software, or other system where the output is dependent on the sequence or timing of other uncontrollable events. It becomes a bug when events do not happen in the order the programmer intended.
#+END_QUOTE

Not bad, not bad! (I say inquisitevly, looking into the distance, my hand on my chin 🤔). I scroll down a little further to the section on race conditions in [[https://en.wikipedia.org/wiki/Race_condition#Software][software]]:

#+BEGIN_QUOTE
Critical race conditions often happen when the processes or threads depend on some shared state. Operations upon shared states are critical sections that must be mutually exclusive. Failure to obey this rule opens up the possibility of corrupting the shared state.
#+END_QUOTE

Well, well, well. What do we have here? SHARED STATE. I wasn't going to talk about /YOU/ for another section, but here you are. For now, let's look at an example of a potential race condition, this time in the Go programming language:

#+BEGIN_SRC Go
package main

import (
  "fmt"
)

var num_rings int = 0

func main() {
  go incrementRings()
  fmt.Print("Number of rings to rule them all: ", num_rings)
}

func incrementRings () {
  num_rings++
}
#+END_SRC

The program above, outputs the following when run:

#+BEGIN_SRC
❯ go run main.go
Number of rings to rule them all: 0
#+END_SRC

NB: /the above Go snippet is a program that can execute in parallel, thanks to Golang's "goroutines" (which I won't get into here). I think it looks a lot cleaner and simpler than working with threads in our C example, but I'm not as familiar with C as Go (and that's not saying much)... so! Bias acknowledged - let's move on to discuss this race condition./

There's a race condition here! In the small program above, we have some global, mutable state - ~num_rings~. When I compile and run the golang program, it executes the main funciton. If we didn't know anything about Go's syntax, we might think that when reading ~main()~ line by line, the expression ~go incrementRings()~ would literally, /go and increment the number of rings/. when I ran it, it did not. Will it ever ? It could, because there's a race condition! This example is very arbitrary - and race conditions usually are not -- they are often hiding deep in layers of code. But our example will do for now.

*** Shared mutable state

From what I've gathered reading and listening, shared mutable state is often at the root of the difficulties surrounding concurrent programming. I think of "shared mutable state" as data structure that can be overwritten, changed, or altered from anywhere in your program. In contrast, immutable data is data that will never be changed -- any "alterations" would involve creating "clones" of data with whatever changes made in the new copy. Your definition may vary. I like this [[ https://benmccormick.org/2016/06/04/what-are-mutable-and-immutable-data-structures-2/][brief post]] describing the differences between mutable and immutable objects in Javascript.

In the example above, we have ~num_rings~ -- a piece of data that stores a number intended to increment. That piece of /state/ (or data) /mutates/ (or changes) over time. Data and how it changes is what makes our programs non-trivial, and furthermore, the introduction of making and instructing change to happen in /parallel/ is where things become difficult.

Many programming languages offer abstractions over handling parallel programming. You've already seen syntactic difference in how languages like Go handles concurrency compared to C; but behind these special grammers are powerful ideas and abstractions. Some popular abstractions/methodologies for performing parallel programming include CSP (communicating sequential processing) the Actor model, Software Transactional Memory.

** Concurrency Models (CSP, Actor Model, STM)

Each of these topics could probably warrant a whole book written on them, and so I think I can only afford myself a shallow dive into some examples of these concurrency models (for now).

NB: /While I remain uninformed on these topics I'd like to at least (for my own curiousity) lean on listing some references. Seeing that this is a personal, living breathing document charting my own understanding, the following notes will likely change as I learn more and gather experience./

At least for my own curiousity, I'd like to explore a) a brief description/history of the concurrency model, b) post some kind of a code example (ideally, writing my own.)

*** Actor Model

Abstractly: I've mostly heard of the actor model in reference to Erlang and Elixir. I think Erlang is probably the most common / popular example of the Actor concurrency model and seem to be lauded as a powerful solution for building distributed and fault tolerant systems. But these are just things that I've heard! I have never written an erlang program, let alone a parallel one.

The wikipedia page for the [[https://en.wikipedia.org/wiki/Actor_model][Actor Model]] hits us with this as it's introduction:

#+BEGIN_QUOTE
The actor model in computer science is a mathematical model of concurrent computation that treats "actors" as the universal primitives of concurrent computation. In response to a message that it receives, an actor can: make local decisions, create more actors, send more messages, and determine how to respond to the next message received. Actors may modify their own private state, but can only affect each other through messages (avoiding the need for any locks).
#+END_QUOTE

This post [[http://www.brianstorti.com/the-actor-model/]["The actor model in 10 minutes"]] kicks off by stating that:

#+BEGIN_QUOTE
An actor is the primitive unit of computation. It’s the thing that receives a message and do some kind of computation based on it.

The idea is very similar to what we have in object-oriented languages: An object receives a message (a method call) and do something depending on which message it receives (which method we are calling).
The main difference is that actors are completely isolated from each other and they will never share memory. It’s also worth noting that an actor can maintain a private state that can never be changed directly by another actor.
#+END_QUOTE

This idea of actors as "primitive" units of ... something, makes me feel like they are a core "thing", native to a programming language like a STRING or an INT.

_Further reading:_

[[http://www.brianstorti.com/the-actor-model/][The Actor Model]]
[[http://learnyousomeerlang.com/the-hitchhikers-guide-to-concurrency][The Hitchhiker's Guide to Concurrency]]
[[https://rocketeer.be/articles/concurrency-in-erlang-scala/][Actors in erlang and scala]]

*** TODO CSP

TODO

*** TODO Software Transactional Memory

TODO

** TODO Concurrency Vs Paralellism

TODO

... Talk about the difficulty of distinguising between the two; different definitions etc.

#+BEGIN_QUOTE
The definitions of "concurrency" and "parallelism" sometimes get mixed up, but they are not the same.

A concurrent system is one that can be in charge of many tasks, although not necessarily it is executing them at the same time. You can think of yourself being in the kitchen cooking: you chop an onion, put it to fry, and while it's being fried you chop a tomato, but you are not doing all of those things at the same time: you distribute your time between those tasks. Parallelism would be to stir fry onions with one hand while with the other one you chop a tomato.

At the moment of this writing, Crystal has concurrency support but not parallelism: several tasks can be executed, and a bit of time will be spent on each of these, but two code paths are never executed at the same exact time.

- https://crystal-lang.org/docs/guides/concurrency.html
#+END_QUOTE

** Research and resources
- [[threads in c][https://www.thegeekstuff.com/2012/04/create-threads-in-linux/]]
- diff between [[https://stackoverflow.com/questions/200469/what-is-the-difference-between-a-process-and-a-thread?rq=1][process and a thread]]
- analogy of [[https://stackoverflow.com/a/5201906/5378196][a friend reading a book]] for describing a thread
- [[https://computing.llnl.gov/tutorials/pthreads/#Thread][Excellent pthread tutorial]]
- [[https://github.com/clojure/core.async/blob/master/examples/walkthrough.clj][core async walkthrough]]

